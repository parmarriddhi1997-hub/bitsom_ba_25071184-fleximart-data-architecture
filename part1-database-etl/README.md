# FlexiMart Database Project
# Part 1: Database Design and ETL Pipeline
## Problem Statement

FlexiMart maintains three raw CSV files containing customer, product, and sales data. These datasets contain multiple data quality issues such as missing values, duplicate records, inconsistent formats, and invalid references.

The objective of this project is to:

- Clean and transform the raw data

- Load it into a relational database (MySQL)

- Design a normalized schema

- Generate a data quality report

- Write SQL business queries for analytical insights

This project demonstrates a complete ETL (Extract–Transform–Load) pipeline and database design following industry best practices.

## Input Data Files
### 1. customers_raw.csv

Contains customer details.

#### Data Quality Issues:

- Missing email values

- Duplicate customer records

- Inconsistent phone number formats

- Mixed-case city names

- Inconsistent date formats

### 2. products_raw.csv

Contains product information.

#### Data Quality Issues:

- Missing product prices

- Missing stock quantities

- Inconsistent category naming

- Extra whitespace in product names

### 3. sales_raw.csv

Contains transactional sales data.

#### Data Quality Issues:

- Duplicate transaction IDs

- Missing customer_id and product_id

- Inconsistent date formats

This file is used to populate both orders and order_items tables.

## Task 1.1: ETL Pipeline Implementation (15 Marks)
### Overview

An automated ETL pipeline (__etl_pipeline.py__) was developed to:

- Extract data from CSV files

- Clean and standardize the data

- Load the cleaned data into a MySQL database

- Generate a data quality report

### ETL Pipeline Design
#### 1. Extract Phase

- Reads all three CSV files using pandas

- Handles missing markers such as empty strings, "NULL", and "None"

#### 2. Transform Phase
##### Duplicate Handling

- Duplicate customer records removed using customer_id

- Duplicate transactions removed using transaction_id

##### Missing Value Handling

- Missing emails replaced with default placeholder values

- Missing prices and stock values replaced with defaults

- Missing customer_id or product_id in sales detected and counted

- Missing values are either dropped or preserved as NULL depending on business logic

##### Standardization

- Phone numbers normalized to format: +91-XXXXXXXXXX

- Category names standardized (e.g., Electronics, Fashion, Groceries)

- City names converted to title case

- Dates converted to YYYY-MM-DD format

##### Surrogate Keys

- Auto-increment primary keys are generated by the database

- Foreign key relationships are preserved during loading

#### 3. Load Phase

Cleaned data is inserted into the MySQL database using Python with proper error handling:

- customers
- products
- orders
- order_items

Foreign key constraints ensure referential integrity.

### Data Quality Report

A file named __data_quality_report.txt__ is automatically generated.

It contains:

- Number of records processed

- Number of duplicate records removed

- Number of missing values handled

- Number of records successfully loaded

Example Output:
Table: customers
Records processed        : 26
Duplicates removed       : 1
Missing values handled   : 5
Records loaded           : 25

Table: products
Records processed        : 20
Duplicates removed       : 0
Missing values handled   : 4
Records loaded           : 20

Table: sales
Records processed        : 41
Duplicates removed       : 1
Missing values handled   : 5
Records loaded           : 35

### Database Schema

The project uses the following normalized schema (provided in the assignment):

#### Tables

- customers
- products
- orders
- order_items

All tables use surrogate primary keys and enforce foreign key relationships.

## Task 1.2: Database Schema Documentation (Summary)
### Entity Relationships

- One customer → many orders

- One order → many order_items

- One product → many order_items

### Normalization (3NF Justification)

The schema satisfies Third Normal Form (3NF):

#### 1NF

- All attributes are atomic

- No repeating groups

#### 2NF

- All non-key attributes depend on the whole primary key

- Composite dependencies are avoided by separating orders and order items

#### 3NF

- No transitive dependencies exist

- Customer details are not stored in the orders table

- Product details are not duplicated in order_items

This design avoids:

- Update anomalies

- Insert anomalies

- Delete anomalies

## Task 1.3: Business Query Implementation (15 Marks)

SQL queries were written in __business_queries.sql__ to answer key business questions.

### Query 1: Customer Purchase History

- Joins customers, orders, and order_items

- Uses GROUP BY and HAVING

- Filters customers with ≥ 2 orders and spending > ₹5000

- Sorted by total spending (descending)

### Query 2: Product Sales Analysis

- Joins products and order_items

- Aggregates quantity and revenue

- Uses COUNT(DISTINCT)

- Filters categories with revenue > ₹10,000

### Query 3: Monthly Sales Trend

- Groups data by month (2024)

- Calculates monthly revenue

- Uses window function for cumulative revenue

- Orders results chronologically

## Technologies Used

- Python (Pandas, NumPy, mysql.connector, parser)

- MySQL

- SQL

- Jupyter Notebook

- CSV files

- Markdown documentation

## Final Deliverables
- requirements.txt 

- etl_pipeline.py

- data_quality_report.txt

- schema_documentation.md

- business_queries.sql

- README.md

## Conclusion

This project demonstrates an end-to-end ETL workflow including data cleaning, normalization, database loading, and analytical querying. The design ensures data integrity, scalability, and efficient reporting suitable for real-world retail analytics.